# 데이터 분석 캡스톤, 최종 발표

## 서론

>문서 요약이란 주어진 문서로부터 자동으로 문서 내의 중요한 내용을 포함하고 있는 짧은 글을 생성하는 것을 말한다. 이전에는 TextRank와 같은 추출적 요약이 산업 및 연구의 주류를 이뤘다. 이후 딥러닝 기법의 발전으로 추상적 요약이 학계에서 주목받고 있으며 특히 Seqence-to-Sequence, Transformer로 대표되는 인코더 - 디코더 방법에 대한 연구, 그리고 attention mechanism에 대한 연구가 활발히 이루어지고 있다.

### 주제 

어텐션 메커니즘을 이용한 인코더-디코더 모델의 음식 리뷰 텍스트 요약 및 성능 평가



### 사용 데이터

<img src="https://user-images.githubusercontent.com/46865281/85838408-83c69200-b7d4-11ea-87f5-5a4dcd367a2c.png"  width="830" height="200">

* [Amazon Fine Food Reviews](https://www.kaggle.com/snap/amazon-fine-food-reviews)

* 102913 행 source와 summary 데이터

* 예시

  | source                                                       | summary               |
  | ------------------------------------------------------------ | --------------------- |
  | I have bought several of the Vitality canned dog food products and have found them all to be of good... | Good Quality Dog Food |

  

## 방법론

### Long Short-Term Memory (LSTM)

* 바닐라  RNN의 문제점: long-term dependencies

<img src="https://t1.daumcdn.net/cfile/tistory/995E00425ACB86A018"  width="500" height="180">

* LSTM

  <img src="https://t1.daumcdn.net/cfile/tistory/999F603E5ACB86A005"  width="500" height="180">



### 인코더-디코더 모델, seq2seq

<img src="https://yjucho1.github.io/assets/img/2018-10-13/encoder-decoder-example.png"  width="500" height="180">[^1]

- 인코더는 입력 시퀀스를 처리하여 고정된 길이의 컨텍스트 벡터로 정보(문맥 정보)를 압축
- 디코더는 컨텍스트 벡터를 다시 처리하여 결과값을 만들어 냄
- 모델의 시퀸스, 순서 조절 가능!



### 어텐션 메커니즘

* 인코더 - 디코더의 문제점
  * 문장 임베딩을 고정된 길이로만 해야 한다는 점
  * 문장이 길어질수록 더 많은 정보를 고정된 길이로 더 많이 압축해야 하기 때문에 정보의 손실 큼
  * "중요한 부분만 집중(attention)하게 만들자"

* 어텐션 메커니즘이란

  > 독일어 “Ich mochte ein bier”를 영어 “I’d like a beer”로 번역하는 S2S 모델을 만든다고 칩시다. 모델이 네번째 단어인 ‘beer’를 예측할 때 ‘bier’에 주목하게 만들고자 합니다. 어텐션 매커니즘의 가정은 **인코더가 ‘bier’를 받아서 벡터로 만든 결과(인코더 출력)는 디코더가 ‘beer’를 예측할 때 쓰는 벡터(디코더 입력)와 유사할 것**이라는 점입니다. - [ratsgo's blog]([https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/10/06/attention/](https://ratsgo.github.io/from frequency to semantics/2017/10/06/attention/))

  * 인코더의 모든 time step에서의 출력(memory 혹은 key)을 Decoder에 전달
    * key.shape = [batch_size, Encoder sequence Length, Hidden Dim]
  * 디코더의 time step i에서의 hidden state(query)
    * query,shape = [batch_size, Hidden Dim]
  * 요약

  <img src="https://github.com/hccho2/Tensorflow-2-RNN-Tutorial/raw/master/2.%20Attention-With-Tensorflow/score.png" width="650" height="450">

* 계산 과정

  * score 계산 
    
    * 디코더가 time_step i번째 단어를 예측할 때 쓰는 직전 스텝의 히든스테이트 벡터가 인코더의 j번째 열벡터와 얼마나 유사한지 나타냄
    
      ![image](https://user-images.githubusercontent.com/46865281/85850197-47e8f800-b7e7-11ea-9535-1a6413835af7.png)

  * score에 softmax 함수를 적용해 합이 1이 되도록 확률값으로 변환
    
    ![image](https://user-images.githubusercontent.com/46865281/85850237-5d5e2200-b7e7-11ea-9ae8-f3f67a667562.png)
    
  * 디코더 예측에 쓰이는 i번째 context vector 가중합
    
    ![image](https://user-images.githubusercontent.com/46865281/85850261-6b13a780-b7e7-11ea-814d-bdb1534d2a64.png)
    
  * 이쁜 그림

    <img src="https://t1.daumcdn.net/cfile/tistory/9955F44C5C066B6909" width="400" height="450">

* [Bahdanau attention](https://arxiv.org/pdf/1409.0473.pdf)  사용!



### ROUGE

* rouge-1
  
  * 1-gram (unigram)
  
* Recall, Precision, F1

  ![image](https://user-images.githubusercontent.com/46865281/85850295-8088d180-b7e7-11ea-85e1-c168c9edc6e8.png)


### 실험 설계

* tf 2.2 keras functional API, [attention_keras](https://github.com/thushv89/attention_keras)

* GPU: titan xp, cuda:10.0, ufoym/deepo iamge 수정

* 어텐션 메커니즘을 적용하지 않은 경우와 적용한 경우, 요약 결과를 ROUGE-1 척도로 평가

  

## 결과

### Model 1

```python
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 50)           0                                            
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 50, 128)      1024000     input_29[0][0]                   
__________________________________________________________________________________________________
lstm_29 (LSTM)                  [(None, 50, 256), (N 394240      embedding_14[0][0]               
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
lstm_30 (LSTM)                  [(None, 50, 256), (N 525312      lstm_29[0][0]                    
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, None, 128)    256000      input_30[0][0]                   
__________________________________________________________________________________________________
lstm_31 (LSTM)                  [(None, 50, 256), (N 525312      lstm_30[0][0]                    
__________________________________________________________________________________________________
lstm_32 (LSTM)                  [(None, None, 256),  394240      embedding_15[0][0]               
                                                                 lstm_31[0][1]                    
                                                                 lstm_31[0][2]                    
__________________________________________________________________________________________________
attention_layer (AttentionLayer [(None, None, 256),  131328      lstm_31[0][0]                    
                                                                 lstm_32[0][0]                    
__________________________________________________________________________________________________
concat_layer (Concatenate)      (None, None, 512)    0           lstm_32[0][0]                    
                                                                 attention_layer[0][0]            
_____________________________________________________________________________________
dense_3 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               
==================================================================================================
Total params: 4,276,432
Trainable params: 4,276,432
Non-trainable params: 0
__________________________________________________________________________________________________
```

* 옵티마이저: [adam](https://keras.io/ko/optimizers/), loss: sparse_categorical_crossentropy 
* embedding_dim = 128, hidden_size = 256, batch_size = 256
* early stopping, 50 epoch

* test

  ```python
  Review :  really like drink complaint would need drink cans time oz wee bit petite side came oz would much likely buy packaging would much less costly really tasty product though got ingredients yum 
  Original summary : tasty but too for me 
  Predicted summary :  great taste
  
  Review :  yummy mild delicious new favorite mild coffees called rocket fuel taste like oil truly medium roast pleasant morning makes glad good cup coffee 
  Original summary : my new favorite 
  Predicted summary :  great coffee
  
  Review :  always purchased star tuna thought would try brand change pace taste tuna pleasant much basil spices 
  Original summary : not the greatest tasting 
  Predicted summary :  not bad
  ```

  

### Model 2

```python
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_29 (InputLayer)           (None, 50)           0                                            
__________________________________________________________________________________________________
embedding_14 (Embedding)        (None, 50, 128)      1024000     input_29[0][0]                   
__________________________________________________________________________________________________
lstm_29 (LSTM)                  [(None, 50, 256), (N 394240      embedding_14[0][0]               
__________________________________________________________________________________________________
input_30 (InputLayer)           (None, None)         0                                            
__________________________________________________________________________________________________
lstm_30 (LSTM)                  [(None, 50, 256), (N 525312      lstm_29[0][0]                    
__________________________________________________________________________________________________
embedding_15 (Embedding)        (None, None, 128)    256000      input_30[0][0]                   
__________________________________________________________________________________________________
lstm_31 (LSTM)                  [(None, 50, 256), (N 525312      lstm_30[0][0]                    
__________________________________________________________________________________________________
lstm_32 (LSTM)                  [(None, None, 256),  394240      embedding_15[0][0]               
                                                                 lstm_31[0][1]                    
                                                                 lstm_31[0][2]                    
__________________________________________________________________________________________________
attention_layer (AttentionLayer [(None, None, 256),  131328      lstm_31[0][0]                    
                                                                 lstm_32[0][0]                    
__________________________________________________________________________________________________
concat_layer (Concatenate)      (None, None, 512)    0           lstm_32[0][0]                    
                                                                 attention_layer[0][0]            
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               
==================================================================================================
Total params: 4,276,432
Trainable params: 4,276,432
Non-trainable params: 0
__________________________________________________________________________________________________
```

* model 1 with bahdanau attention

* test

  ```python
  Review : perfect stress free afternoon aroma tea makes house smell great drink grade honey bliss 
  Original summary : relax cup of tea 
  Predicted summary: great tea
  
  Review : got bbq popchips amazon promotion price came taste good wish less salty would certainly purchase came less salty version 
  Original summary : tasty but wish it was less salty 
  Predicted summary : not the best
  
  Review : product arrived broken pieces flavor good actually threw garbage disappointing 
  Original summary : very disappointed 
  Predicted summary: not as described
  ```



### Evaluation

|         | precision | recall | F1 score            |
| ------- | --------- | ------ | ------------------- |
| model 1 | 0.141     | 0.107  | 0.114               |
| model 2 | 0.161     | 0.119  | 0.12840433237005155 |



[^1]:https://yjucho1.github.io/attention/attention/

